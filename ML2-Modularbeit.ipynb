{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8168e98c",
   "metadata": {},
   "source": [
    "# Timon Spichtinger Machine Learning 2 Modularbeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "113ca418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timon/ML2Modularbeit/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import EMNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import importlib\n",
    "from Datensatz import get_emnist_test_train, show_random_samples,create_class_type_map, create_type_labels\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from Training import klassifier_training\n",
    "from Training import train_type_classifier\n",
    "from Klassifikator import EarlyStopping, ModularClassifier,TypeClassifier, ResNet18, get_objective, train_one_epoch_type, evaluate_model_type, compute_accuracy_type, train_one_epoch_modular, evaluate_model_modular, compute_accuracy_modular\n",
    "import torch.optim as optim\n",
    "from Evaluation import evaluate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddf5338",
   "metadata": {},
   "source": [
    "## 1.1 Datensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3bf0cf",
   "metadata": {},
   "source": [
    "Train und Testdaten werden aus Emnist-Datensatz geladen. Falls es zuwenige gibt werden die restlichen Augmentiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b5f04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ziel-ASCII: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109]\n",
      "Anzahl Zielklassen: 36\n",
      "⚠️ Klasse B: nur 3878 echte Bilder – augmentiere 2122 zusätzlich.\n",
      "⚠️ Klasse D: nur 4562 echte Bilder – augmentiere 1438 zusätzlich.\n",
      "⚠️ Klasse E: nur 4934 echte Bilder – augmentiere 1066 zusätzlich.\n",
      "⚠️ Klasse G: nur 2517 echte Bilder – augmentiere 3483 zusätzlich.\n",
      "⚠️ Klasse H: nur 3152 echte Bilder – augmentiere 2848 zusätzlich.\n",
      "⚠️ Klasse J: nur 3762 echte Bilder – augmentiere 2238 zusätzlich.\n",
      "⚠️ Klasse K: nur 2468 echte Bilder – augmentiere 3532 zusätzlich.\n",
      "⚠️ Klasse L: nur 5076 echte Bilder – augmentiere 924 zusätzlich.\n",
      "⚠️ Klasse b: nur 5159 echte Bilder – augmentiere 841 zusätzlich.\n",
      "⚠️ Klasse c: nur 2854 echte Bilder – augmentiere 3146 zusätzlich.\n",
      "⚠️ Klasse f: nur 2561 echte Bilder – augmentiere 3439 zusätzlich.\n",
      "⚠️ Klasse g: nur 3687 echte Bilder – augmentiere 2313 zusätzlich.\n",
      "⚠️ Klasse i: nur 2725 echte Bilder – augmentiere 3275 zusätzlich.\n",
      "⚠️ Klasse j: nur 1896 echte Bilder – augmentiere 4104 zusätzlich.\n",
      "⚠️ Klasse k: nur 2491 echte Bilder – augmentiere 3509 zusätzlich.\n",
      "⚠️ Klasse m: nur 2645 echte Bilder – augmentiere 3355 zusätzlich.\n",
      "✅ Trainingsdaten: torch.Size([180000, 1, 28, 28]) torch.Size([180000])\n",
      "✅ Testdaten: torch.Size([36000, 1, 28, 28]) torch.Size([36000])\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test,class_list = get_emnist_test_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58605ffa",
   "metadata": {},
   "source": [
    "Stichprobe, ob der Datensatz passt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39fe5cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "show_random_samples(X_test, y_test, class_list, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ae09a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5470d665",
   "metadata": {},
   "source": [
    "## 1.2 Klassifikator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5449b0aa",
   "metadata": {},
   "source": [
    "Im Folgenden soll ein Klassifikator trainiert werden, der die EMNIST-Daten klassifiziert. Ich habe ein Resnet benutzt, da sie keine schwierigkeitem bei höheren Epochen haben"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa29f5",
   "metadata": {},
   "source": [
    "Gerät initialisieren und Tensordatasets erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c18fcdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gerät\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce5e1f5",
   "metadata": {},
   "source": [
    "Die Optina-Studie gibt uns die besten Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c7b06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Optuna-Studie starten\n",
    "# -----------------------------\n",
    "\"\"\"\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\",\n",
    "                             storage=\"sqlite:///optuna_studies.db\",\n",
    "                            study_name=\"resnet_hyperparam_search4\")\n",
    "study.optimize(get_objective(\n",
    "          train_dataset=train_dataset,\n",
    "          test_dataset=test_dataset,\n",
    "          device=device,\n",
    "          model=ResNet18(num_classes=len(class_list)).to(device),\n",
    "          early_stopping=EarlyStopping(),\n",
    "          train_one_epoch), n_trials=20)\n",
    "\n",
    "# Beste Parameter anzeigen\n",
    "print(\"🎯 Beste Hyperparameter:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "\"\"\"\n",
    "study = optuna.load_study(\n",
    "    study_name=\"modular_model_hyperparam_search4\",\n",
    "    storage=\"sqlite:///optuna_studies.db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdf5b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Parameter für Finales Training \n",
    "# -----------------------------\n",
    "best_params = study.best_params\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "model = ResNet18().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=best_params[\"lr\"], momentum=best_params[\"momentum\"])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=best_params[\"step_size\"], gamma=best_params[\"gamma\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341149e1",
   "metadata": {},
   "source": [
    "Training des Klassifikators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e123029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Epoch 1, Step 200/1407: Batch Loss = 3.0965\n",
      "🔁 Epoch 1, Step 400/1407: Batch Loss = 2.4888\n",
      "🔁 Epoch 1, Step 600/1407: Batch Loss = 2.0685\n",
      "🔁 Epoch 1, Step 800/1407: Batch Loss = 1.5796\n",
      "🔁 Epoch 1, Step 1000/1407: Batch Loss = 1.2790\n",
      "🔁 Epoch 1, Step 1200/1407: Batch Loss = 1.1367\n",
      "🔁 Epoch 1, Step 1400/1407: Batch Loss = 0.8745\n",
      "📊 Epoch 1: Train Acc = 52.81%, Val Acc = 74.32%, Val Loss = 0.8894, LR = [0.00022409484399187092]\n",
      "Validation loss decreased (inf --> 0.8894). Saving model state.\n",
      "🔁 Epoch 2, Step 200/1407: Batch Loss = 0.7995\n",
      "🔁 Epoch 2, Step 400/1407: Batch Loss = 0.6928\n",
      "🔁 Epoch 2, Step 600/1407: Batch Loss = 0.6277\n",
      "🔁 Epoch 2, Step 800/1407: Batch Loss = 0.8181\n",
      "🔁 Epoch 2, Step 1000/1407: Batch Loss = 0.5904\n",
      "🔁 Epoch 2, Step 1200/1407: Batch Loss = 0.6634\n",
      "🔁 Epoch 2, Step 1400/1407: Batch Loss = 0.4059\n",
      "📊 Epoch 2: Train Acc = 78.72%, Val Acc = 81.52%, Val Loss = 0.5489, LR = [0.00022409484399187092]\n",
      "Validation loss decreased (0.8894 --> 0.5489). Saving model state.\n",
      "🔁 Epoch 3, Step 200/1407: Batch Loss = 0.5817\n",
      "🔁 Epoch 3, Step 400/1407: Batch Loss = 0.5186\n",
      "🔁 Epoch 3, Step 600/1407: Batch Loss = 0.4840\n",
      "🔁 Epoch 3, Step 800/1407: Batch Loss = 0.4701\n",
      "🔁 Epoch 3, Step 1000/1407: Batch Loss = 0.5442\n",
      "🔁 Epoch 3, Step 1200/1407: Batch Loss = 0.4621\n",
      "🔁 Epoch 3, Step 1400/1407: Batch Loss = 0.4925\n",
      "📊 Epoch 3: Train Acc = 82.68%, Val Acc = 83.40%, Val Loss = 0.4620, LR = [0.00013660344077009117]\n",
      "Validation loss decreased (0.5489 --> 0.4620). Saving model state.\n",
      "🔁 Epoch 4, Step 200/1407: Batch Loss = 0.4095\n",
      "🔁 Epoch 4, Step 400/1407: Batch Loss = 0.4877\n",
      "🔁 Epoch 4, Step 600/1407: Batch Loss = 0.3950\n",
      "🔁 Epoch 4, Step 800/1407: Batch Loss = 0.3267\n",
      "🔁 Epoch 4, Step 1000/1407: Batch Loss = 0.2856\n",
      "🔁 Epoch 4, Step 1200/1407: Batch Loss = 0.5093\n",
      "🔁 Epoch 4, Step 1400/1407: Batch Loss = 0.4542\n",
      "📊 Epoch 4: Train Acc = 84.21%, Val Acc = 84.21%, Val Loss = 0.4327, LR = [0.00013660344077009117]\n",
      "Validation loss decreased (0.4620 --> 0.4327). Saving model state.\n",
      "🔁 Epoch 5, Step 200/1407: Batch Loss = 0.4501\n",
      "🔁 Epoch 5, Step 400/1407: Batch Loss = 0.3650\n",
      "🔁 Epoch 5, Step 600/1407: Batch Loss = 0.3752\n",
      "🔁 Epoch 5, Step 800/1407: Batch Loss = 0.3514\n",
      "🔁 Epoch 5, Step 1000/1407: Batch Loss = 0.4085\n",
      "🔁 Epoch 5, Step 1200/1407: Batch Loss = 0.3454\n",
      "🔁 Epoch 5, Step 1400/1407: Batch Loss = 0.3995\n",
      "📊 Epoch 5: Train Acc = 84.96%, Val Acc = 84.82%, Val Loss = 0.4056, LR = [0.00013660344077009117]\n",
      "Validation loss decreased (0.4327 --> 0.4056). Saving model state.\n",
      "🔁 Epoch 6, Step 200/1407: Batch Loss = 0.3280\n",
      "🔁 Epoch 6, Step 400/1407: Batch Loss = 0.3849\n",
      "🔁 Epoch 6, Step 600/1407: Batch Loss = 0.3289\n",
      "🔁 Epoch 6, Step 800/1407: Batch Loss = 0.3470\n",
      "🔁 Epoch 6, Step 1000/1407: Batch Loss = 0.2931\n",
      "🔁 Epoch 6, Step 1200/1407: Batch Loss = 0.2789\n",
      "🔁 Epoch 6, Step 1400/1407: Batch Loss = 0.3681\n",
      "📊 Epoch 6: Train Acc = 85.56%, Val Acc = 84.64%, Val Loss = 0.3992, LR = [8.327054606800645e-05]\n",
      "Validation loss decreased (0.4056 --> 0.3992). Saving model state.\n",
      "🔁 Epoch 7, Step 200/1407: Batch Loss = 0.3225\n",
      "🔁 Epoch 7, Step 400/1407: Batch Loss = 0.3506\n",
      "🔁 Epoch 7, Step 600/1407: Batch Loss = 0.3698\n",
      "🔁 Epoch 7, Step 800/1407: Batch Loss = 0.3451\n",
      "🔁 Epoch 7, Step 1000/1407: Batch Loss = 0.3946\n",
      "🔁 Epoch 7, Step 1200/1407: Batch Loss = 0.3877\n",
      "🔁 Epoch 7, Step 1400/1407: Batch Loss = 0.3997\n",
      "📊 Epoch 7: Train Acc = 86.00%, Val Acc = 85.29%, Val Loss = 0.3829, LR = [8.327054606800645e-05]\n",
      "Validation loss decreased (0.3992 --> 0.3829). Saving model state.\n",
      "🔁 Epoch 8, Step 200/1407: Batch Loss = 0.3202\n",
      "🔁 Epoch 8, Step 400/1407: Batch Loss = 0.3426\n",
      "🔁 Epoch 8, Step 600/1407: Batch Loss = 0.3670\n",
      "🔁 Epoch 8, Step 800/1407: Batch Loss = 0.2736\n",
      "🔁 Epoch 8, Step 1000/1407: Batch Loss = 0.3372\n",
      "🔁 Epoch 8, Step 1200/1407: Batch Loss = 0.4489\n",
      "🔁 Epoch 8, Step 1400/1407: Batch Loss = 0.2668\n",
      "📊 Epoch 8: Train Acc = 86.25%, Val Acc = 85.45%, Val Loss = 0.3755, LR = [8.327054606800645e-05]\n",
      "Validation loss decreased (0.3829 --> 0.3755). Saving model state.\n",
      "🔁 Epoch 9, Step 200/1407: Batch Loss = 0.3033\n",
      "🔁 Epoch 9, Step 400/1407: Batch Loss = 0.2577\n",
      "🔁 Epoch 9, Step 600/1407: Batch Loss = 0.3817\n",
      "🔁 Epoch 9, Step 800/1407: Batch Loss = 0.3776\n",
      "🔁 Epoch 9, Step 1000/1407: Batch Loss = 0.3531\n",
      "🔁 Epoch 9, Step 1200/1407: Batch Loss = 0.4498\n",
      "🔁 Epoch 9, Step 1400/1407: Batch Loss = 0.3743\n",
      "📊 Epoch 9: Train Acc = 86.50%, Val Acc = 85.58%, Val Loss = 0.3715, LR = [5.075995014015896e-05]\n",
      "Validation loss decreased (0.3755 --> 0.3715). Saving model state.\n",
      "🔁 Epoch 10, Step 200/1407: Batch Loss = 0.2412\n",
      "🔁 Epoch 10, Step 400/1407: Batch Loss = 0.4390\n",
      "🔁 Epoch 10, Step 600/1407: Batch Loss = 0.3600\n",
      "🔁 Epoch 10, Step 800/1407: Batch Loss = 0.3654\n",
      "🔁 Epoch 10, Step 1000/1407: Batch Loss = 0.4418\n",
      "🔁 Epoch 10, Step 1200/1407: Batch Loss = 0.3097\n",
      "🔁 Epoch 10, Step 1400/1407: Batch Loss = 0.4485\n",
      "📊 Epoch 10: Train Acc = 86.79%, Val Acc = 85.70%, Val Loss = 0.3693, LR = [5.075995014015896e-05]\n",
      "Validation loss decreased (0.3715 --> 0.3693). Saving model state.\n",
      "🔁 Epoch 11, Step 200/1407: Batch Loss = 0.3500\n",
      "🔁 Epoch 11, Step 400/1407: Batch Loss = 0.2678\n",
      "🔁 Epoch 11, Step 600/1407: Batch Loss = 0.3462\n",
      "🔁 Epoch 11, Step 800/1407: Batch Loss = 0.2693\n",
      "🔁 Epoch 11, Step 1000/1407: Batch Loss = 0.2083\n",
      "🔁 Epoch 11, Step 1200/1407: Batch Loss = 0.3658\n",
      "🔁 Epoch 11, Step 1400/1407: Batch Loss = 0.2983\n",
      "📊 Epoch 11: Train Acc = 86.97%, Val Acc = 85.70%, Val Loss = 0.3672, LR = [5.075995014015896e-05]\n",
      "Validation loss decreased (0.3693 --> 0.3672). Saving model state.\n",
      "🔁 Epoch 12, Step 200/1407: Batch Loss = 0.3295\n",
      "🔁 Epoch 12, Step 400/1407: Batch Loss = 0.3995\n",
      "🔁 Epoch 12, Step 600/1407: Batch Loss = 0.3463\n",
      "🔁 Epoch 12, Step 800/1407: Batch Loss = 0.3181\n",
      "🔁 Epoch 12, Step 1000/1407: Batch Loss = 0.3096\n",
      "🔁 Epoch 12, Step 1200/1407: Batch Loss = 0.3512\n",
      "🔁 Epoch 12, Step 1400/1407: Batch Loss = 0.3184\n",
      "📊 Epoch 12: Train Acc = 87.03%, Val Acc = 85.72%, Val Loss = 0.3645, LR = [3.0942183759995464e-05]\n",
      "Validation loss decreased (0.3672 --> 0.3645). Saving model state.\n",
      "🔁 Epoch 13, Step 200/1407: Batch Loss = 0.4216\n",
      "🔁 Epoch 13, Step 400/1407: Batch Loss = 0.3451\n",
      "🔁 Epoch 13, Step 600/1407: Batch Loss = 0.3408\n",
      "🔁 Epoch 13, Step 800/1407: Batch Loss = 0.3154\n",
      "🔁 Epoch 13, Step 1000/1407: Batch Loss = 0.3723\n",
      "🔁 Epoch 13, Step 1200/1407: Batch Loss = 0.3546\n",
      "🔁 Epoch 13, Step 1400/1407: Batch Loss = 0.3263\n",
      "📊 Epoch 13: Train Acc = 87.25%, Val Acc = 85.77%, Val Loss = 0.3636, LR = [3.0942183759995464e-05]\n",
      "Validation loss decreased (0.3645 --> 0.3636). Saving model state.\n",
      "🔁 Epoch 14, Step 200/1407: Batch Loss = 0.2557\n",
      "🔁 Epoch 14, Step 400/1407: Batch Loss = 0.3807\n",
      "🔁 Epoch 14, Step 600/1407: Batch Loss = 0.3659\n",
      "🔁 Epoch 14, Step 800/1407: Batch Loss = 0.4318\n",
      "🔁 Epoch 14, Step 1000/1407: Batch Loss = 0.4502\n",
      "🔁 Epoch 14, Step 1200/1407: Batch Loss = 0.2628\n",
      "🔁 Epoch 14, Step 1400/1407: Batch Loss = 0.3141\n",
      "📊 Epoch 14: Train Acc = 87.38%, Val Acc = 85.81%, Val Loss = 0.3617, LR = [3.0942183759995464e-05]\n",
      "Validation loss decreased (0.3636 --> 0.3617). Saving model state.\n",
      "🔁 Epoch 15, Step 200/1407: Batch Loss = 0.3661\n",
      "🔁 Epoch 15, Step 400/1407: Batch Loss = 0.2480\n",
      "🔁 Epoch 15, Step 600/1407: Batch Loss = 0.3495\n",
      "🔁 Epoch 15, Step 800/1407: Batch Loss = 0.2695\n",
      "🔁 Epoch 15, Step 1000/1407: Batch Loss = 0.3057\n",
      "🔁 Epoch 15, Step 1200/1407: Batch Loss = 0.3140\n",
      "🔁 Epoch 15, Step 1400/1407: Batch Loss = 0.3289\n",
      "📊 Epoch 15: Train Acc = 87.41%, Val Acc = 85.85%, Val Loss = 0.3605, LR = [1.8861695750167038e-05]\n",
      "Validation loss decreased (0.3617 --> 0.3605). Saving model state.\n",
      "🔁 Epoch 16, Step 200/1407: Batch Loss = 0.2538\n",
      "🔁 Epoch 16, Step 400/1407: Batch Loss = 0.3281\n",
      "🔁 Epoch 16, Step 600/1407: Batch Loss = 0.2593\n",
      "🔁 Epoch 16, Step 800/1407: Batch Loss = 0.3828\n",
      "🔁 Epoch 16, Step 1000/1407: Batch Loss = 0.2950\n",
      "🔁 Epoch 16, Step 1200/1407: Batch Loss = 0.3869\n",
      "🔁 Epoch 16, Step 1400/1407: Batch Loss = 0.3116\n",
      "📊 Epoch 16: Train Acc = 87.55%, Val Acc = 85.93%, Val Loss = 0.3595, LR = [1.8861695750167038e-05]\n",
      "Validation loss decreased (0.3605 --> 0.3595). Saving model state.\n",
      "🔁 Epoch 17, Step 200/1407: Batch Loss = 0.3620\n",
      "🔁 Epoch 17, Step 400/1407: Batch Loss = 0.3405\n",
      "🔁 Epoch 17, Step 600/1407: Batch Loss = 0.4043\n",
      "🔁 Epoch 17, Step 800/1407: Batch Loss = 0.3009\n",
      "🔁 Epoch 17, Step 1000/1407: Batch Loss = 0.2588\n",
      "🔁 Epoch 17, Step 1200/1407: Batch Loss = 0.2991\n",
      "🔁 Epoch 17, Step 1400/1407: Batch Loss = 0.2416\n",
      "📊 Epoch 17: Train Acc = 87.62%, Val Acc = 85.92%, Val Loss = 0.3590, LR = [1.8861695750167038e-05]\n",
      "Validation loss decreased (0.3595 --> 0.3590). Saving model state.\n",
      "🔁 Epoch 18, Step 200/1407: Batch Loss = 0.2704\n",
      "🔁 Epoch 18, Step 400/1407: Batch Loss = 0.3375\n",
      "🔁 Epoch 18, Step 600/1407: Batch Loss = 0.2908\n",
      "🔁 Epoch 18, Step 800/1407: Batch Loss = 0.2804\n",
      "🔁 Epoch 18, Step 1000/1407: Batch Loss = 0.4460\n",
      "🔁 Epoch 18, Step 1200/1407: Batch Loss = 0.3723\n",
      "🔁 Epoch 18, Step 1400/1407: Batch Loss = 0.2985\n",
      "📊 Epoch 18: Train Acc = 87.71%, Val Acc = 85.91%, Val Loss = 0.3586, LR = [1.1497687730490082e-05]\n",
      "Validation loss decreased (0.3590 --> 0.3586). Saving model state.\n",
      "🔁 Epoch 19, Step 200/1407: Batch Loss = 0.2586\n",
      "🔁 Epoch 19, Step 400/1407: Batch Loss = 0.3092\n",
      "🔁 Epoch 19, Step 600/1407: Batch Loss = 0.2322\n",
      "🔁 Epoch 19, Step 800/1407: Batch Loss = 0.3595\n",
      "🔁 Epoch 19, Step 1000/1407: Batch Loss = 0.3796\n",
      "🔁 Epoch 19, Step 1200/1407: Batch Loss = 0.4254\n",
      "🔁 Epoch 19, Step 1400/1407: Batch Loss = 0.3239\n",
      "📊 Epoch 19: Train Acc = 87.74%, Val Acc = 85.90%, Val Loss = 0.3582, LR = [1.1497687730490082e-05]\n",
      "Validation loss decreased (0.3586 --> 0.3582). Saving model state.\n",
      "🔁 Epoch 20, Step 200/1407: Batch Loss = 0.3258\n",
      "🔁 Epoch 20, Step 400/1407: Batch Loss = 0.3870\n",
      "🔁 Epoch 20, Step 600/1407: Batch Loss = 0.2191\n",
      "🔁 Epoch 20, Step 800/1407: Batch Loss = 0.3227\n",
      "🔁 Epoch 20, Step 1000/1407: Batch Loss = 0.2822\n",
      "🔁 Epoch 20, Step 1200/1407: Batch Loss = 0.2925\n",
      "🔁 Epoch 20, Step 1400/1407: Batch Loss = 0.3355\n",
      "📊 Epoch 20: Train Acc = 87.77%, Val Acc = 85.96%, Val Loss = 0.3574, LR = [1.1497687730490082e-05]\n",
      "Validation loss decreased (0.3582 --> 0.3574). Saving model state.\n",
      "🔁 Epoch 21, Step 200/1407: Batch Loss = 0.3020\n",
      "🔁 Epoch 21, Step 400/1407: Batch Loss = 0.2548\n",
      "🔁 Epoch 21, Step 600/1407: Batch Loss = 0.3598\n",
      "🔁 Epoch 21, Step 800/1407: Batch Loss = 0.2820\n",
      "🔁 Epoch 21, Step 1000/1407: Batch Loss = 0.3201\n",
      "🔁 Epoch 21, Step 1200/1407: Batch Loss = 0.3111\n",
      "🔁 Epoch 21, Step 1400/1407: Batch Loss = 0.2717\n",
      "📊 Epoch 21: Train Acc = 87.80%, Val Acc = 86.00%, Val Loss = 0.3570, LR = [7.008745390598904e-06]\n",
      "Validation loss decreased (0.3574 --> 0.3570). Saving model state.\n",
      "🔁 Epoch 22, Step 200/1407: Batch Loss = 0.2783\n",
      "🔁 Epoch 22, Step 400/1407: Batch Loss = 0.3590\n",
      "🔁 Epoch 22, Step 600/1407: Batch Loss = 0.2904\n",
      "🔁 Epoch 22, Step 800/1407: Batch Loss = 0.3455\n",
      "🔁 Epoch 22, Step 1000/1407: Batch Loss = 0.2954\n",
      "🔁 Epoch 22, Step 1200/1407: Batch Loss = 0.3356\n",
      "🔁 Epoch 22, Step 1400/1407: Batch Loss = 0.3042\n",
      "📊 Epoch 22: Train Acc = 87.80%, Val Acc = 85.96%, Val Loss = 0.3567, LR = [7.008745390598904e-06]\n",
      "Validation loss decreased (0.3570 --> 0.3567). Saving model state.\n",
      "🔁 Epoch 23, Step 200/1407: Batch Loss = 0.2840\n",
      "🔁 Epoch 23, Step 400/1407: Batch Loss = 0.3502\n",
      "🔁 Epoch 23, Step 600/1407: Batch Loss = 0.2847\n",
      "🔁 Epoch 23, Step 800/1407: Batch Loss = 0.2921\n",
      "🔁 Epoch 23, Step 1000/1407: Batch Loss = 0.3157\n",
      "🔁 Epoch 23, Step 1200/1407: Batch Loss = 0.4492\n",
      "🔁 Epoch 23, Step 1400/1407: Batch Loss = 0.3471\n",
      "📊 Epoch 23: Train Acc = 87.88%, Val Acc = 85.99%, Val Loss = 0.3573, LR = [7.008745390598904e-06]\n",
      "EarlyStopping counter: 1/10\n",
      "🔁 Epoch 24, Step 200/1407: Batch Loss = 0.3060\n",
      "🔁 Epoch 24, Step 400/1407: Batch Loss = 0.3290\n",
      "🔁 Epoch 24, Step 600/1407: Batch Loss = 0.2420\n",
      "🔁 Epoch 24, Step 800/1407: Batch Loss = 0.3080\n",
      "🔁 Epoch 24, Step 1000/1407: Batch Loss = 0.2963\n",
      "🔁 Epoch 24, Step 1200/1407: Batch Loss = 0.2570\n",
      "🔁 Epoch 24, Step 1400/1407: Batch Loss = 0.3515\n",
      "📊 Epoch 24: Train Acc = 87.89%, Val Acc = 85.96%, Val Loss = 0.3571, LR = [4.272381812908009e-06]\n",
      "EarlyStopping counter: 2/10\n",
      "🔁 Epoch 25, Step 200/1407: Batch Loss = 0.2928\n",
      "🔁 Epoch 25, Step 400/1407: Batch Loss = 0.3080\n",
      "🔁 Epoch 25, Step 600/1407: Batch Loss = 0.3382\n",
      "🔁 Epoch 25, Step 800/1407: Batch Loss = 0.3327\n",
      "🔁 Epoch 25, Step 1000/1407: Batch Loss = 0.2396\n",
      "🔁 Epoch 25, Step 1200/1407: Batch Loss = 0.2907\n",
      "🔁 Epoch 25, Step 1400/1407: Batch Loss = 0.3698\n",
      "📊 Epoch 25: Train Acc = 87.90%, Val Acc = 86.03%, Val Loss = 0.3563, LR = [4.272381812908009e-06]\n",
      "Validation loss decreased (0.3567 --> 0.3563). Saving model state.\n",
      "🔁 Epoch 26, Step 200/1407: Batch Loss = 0.2811\n",
      "🔁 Epoch 26, Step 400/1407: Batch Loss = 0.3705\n",
      "🔁 Epoch 26, Step 600/1407: Batch Loss = 0.3255\n",
      "🔁 Epoch 26, Step 800/1407: Batch Loss = 0.3514\n",
      "🔁 Epoch 26, Step 1000/1407: Batch Loss = 0.1496\n",
      "🔁 Epoch 26, Step 1200/1407: Batch Loss = 0.3886\n",
      "🔁 Epoch 26, Step 1400/1407: Batch Loss = 0.2728\n",
      "📊 Epoch 26: Train Acc = 87.90%, Val Acc = 86.00%, Val Loss = 0.3564, LR = [4.272381812908009e-06]\n",
      "EarlyStopping counter: 1/10\n",
      "🔁 Epoch 27, Step 200/1407: Batch Loss = 0.3181\n",
      "🔁 Epoch 27, Step 400/1407: Batch Loss = 0.2995\n",
      "🔁 Epoch 27, Step 600/1407: Batch Loss = 0.3939\n",
      "🔁 Epoch 27, Step 800/1407: Batch Loss = 0.3463\n",
      "🔁 Epoch 27, Step 1000/1407: Batch Loss = 0.3242\n",
      "🔁 Epoch 27, Step 1200/1407: Batch Loss = 0.3647\n",
      "🔁 Epoch 27, Step 1400/1407: Batch Loss = 0.3148\n",
      "📊 Epoch 27: Train Acc = 87.98%, Val Acc = 85.87%, Val Loss = 0.3572, LR = [2.6043528959906143e-06]\n",
      "EarlyStopping counter: 2/10\n",
      "🔁 Epoch 28, Step 200/1407: Batch Loss = 0.2752\n",
      "🔁 Epoch 28, Step 400/1407: Batch Loss = 0.2538\n",
      "🔁 Epoch 28, Step 600/1407: Batch Loss = 0.3435\n",
      "🔁 Epoch 28, Step 800/1407: Batch Loss = 0.3359\n",
      "🔁 Epoch 28, Step 1000/1407: Batch Loss = 0.3227\n",
      "🔁 Epoch 28, Step 1200/1407: Batch Loss = 0.2906\n",
      "🔁 Epoch 28, Step 1400/1407: Batch Loss = 0.4080\n",
      "📊 Epoch 28: Train Acc = 87.95%, Val Acc = 86.01%, Val Loss = 0.3562, LR = [2.6043528959906143e-06]\n",
      "Validation loss decreased (0.3563 --> 0.3562). Saving model state.\n",
      "🔁 Epoch 29, Step 200/1407: Batch Loss = 0.2716\n",
      "🔁 Epoch 29, Step 400/1407: Batch Loss = 0.3466\n",
      "🔁 Epoch 29, Step 600/1407: Batch Loss = 0.3615\n",
      "🔁 Epoch 29, Step 800/1407: Batch Loss = 0.3814\n",
      "🔁 Epoch 29, Step 1000/1407: Batch Loss = 0.2389\n",
      "🔁 Epoch 29, Step 1200/1407: Batch Loss = 0.3048\n",
      "🔁 Epoch 29, Step 1400/1407: Batch Loss = 0.2666\n",
      "📊 Epoch 29: Train Acc = 87.97%, Val Acc = 86.01%, Val Loss = 0.3565, LR = [2.6043528959906143e-06]\n",
      "EarlyStopping counter: 1/10\n",
      "🔁 Epoch 30, Step 200/1407: Batch Loss = 0.2721\n",
      "🔁 Epoch 30, Step 400/1407: Batch Loss = 0.3256\n",
      "🔁 Epoch 30, Step 600/1407: Batch Loss = 0.2034\n",
      "🔁 Epoch 30, Step 800/1407: Batch Loss = 0.3563\n",
      "🔁 Epoch 30, Step 1000/1407: Batch Loss = 0.3667\n",
      "🔁 Epoch 30, Step 1200/1407: Batch Loss = 0.2542\n",
      "🔁 Epoch 30, Step 1400/1407: Batch Loss = 0.2518\n",
      "📊 Epoch 30: Train Acc = 87.97%, Val Acc = 86.03%, Val Loss = 0.3561, LR = [1.5875580188929947e-06]\n",
      "Validation loss decreased (0.3562 --> 0.3561). Saving model state.\n",
      "✅ Bestes Modell wurde wiederhergestellt\n"
     ]
    }
   ],
   "source": [
    "klassifier_training(train_loader,test_loader,model,device,criterion,optimizer,scheduler,30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bef864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modell gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# Modell speichern\n",
    "\"\"\"\n",
    "torch.save(model.state_dict(), './resnet18_best_hyperparams.pth')\n",
    "print(\"✅ Modell gespeichert.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a42922",
   "metadata": {},
   "source": [
    "Evaluierung des Ersten Klassifikators. Es fällt auf das Symbole, die sich ähneln (z.B. 1,I,L), schlechter Vorhergesagt werden können"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4baf65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gesamtgenauigkeit des Netzwerks: 87.14 %\n",
      "Genauigkeit für Klasse 0: 97.10 %\n",
      "Genauigkeit für Klasse 1: 59.10 %\n",
      "Genauigkeit für Klasse 2: 96.70 %\n",
      "Genauigkeit für Klasse 3: 97.60 %\n",
      "Genauigkeit für Klasse 4: 98.10 %\n",
      "Genauigkeit für Klasse 5: 98.30 %\n",
      "Genauigkeit für Klasse 6: 94.60 %\n",
      "Genauigkeit für Klasse 7: 98.90 %\n",
      "Genauigkeit für Klasse 8: 96.90 %\n",
      "Genauigkeit für Klasse 9: 95.40 %\n",
      "Genauigkeit für Klasse A: 99.10 %\n",
      "Genauigkeit für Klasse B: 95.50 %\n",
      "Genauigkeit für Klasse C: 87.80 %\n",
      "Genauigkeit für Klasse D: 97.40 %\n",
      "Genauigkeit für Klasse E: 97.00 %\n",
      "Genauigkeit für Klasse F: 86.80 %\n",
      "Genauigkeit für Klasse G: 93.90 %\n",
      "Genauigkeit für Klasse H: 97.70 %\n",
      "Genauigkeit für Klasse I: 61.40 %\n",
      "Genauigkeit für Klasse J: 84.20 %\n",
      "Genauigkeit für Klasse K: 74.60 %\n",
      "Genauigkeit für Klasse L: 96.00 %\n",
      "Genauigkeit für Klasse M: 89.10 %\n",
      "Genauigkeit für Klasse a: 95.20 %\n",
      "Genauigkeit für Klasse b: 93.50 %\n",
      "Genauigkeit für Klasse c: 68.50 %\n",
      "Genauigkeit für Klasse d: 98.80 %\n",
      "Genauigkeit für Klasse e: 97.80 %\n",
      "Genauigkeit für Klasse f: 74.40 %\n",
      "Genauigkeit für Klasse g: 78.30 %\n",
      "Genauigkeit für Klasse h: 97.80 %\n",
      "Genauigkeit für Klasse i: 71.90 %\n",
      "Genauigkeit für Klasse j: 78.60 %\n",
      "Genauigkeit für Klasse k: 69.80 %\n",
      "Genauigkeit für Klasse l: 47.20 %\n",
      "Genauigkeit für Klasse m: 72.20 %\n",
      "\n",
      "Precision, Recall, F1-Score pro Klasse:\n",
      "Klasse 0: Precision=0.97, Recall=0.97, F1-Score=0.97\n",
      "Klasse 1: Precision=0.50, Recall=0.59, F1-Score=0.54\n",
      "Klasse 2: Precision=0.98, Recall=0.97, F1-Score=0.97\n",
      "Klasse 3: Precision=0.99, Recall=0.98, F1-Score=0.98\n",
      "Klasse 4: Precision=0.99, Recall=0.98, F1-Score=0.98\n",
      "Klasse 5: Precision=0.98, Recall=0.98, F1-Score=0.98\n",
      "Klasse 6: Precision=0.96, Recall=0.95, F1-Score=0.95\n",
      "Klasse 7: Precision=0.98, Recall=0.99, F1-Score=0.99\n",
      "Klasse 8: Precision=0.97, Recall=0.97, F1-Score=0.97\n",
      "Klasse 9: Precision=0.89, Recall=0.95, F1-Score=0.92\n",
      "Klasse A: Precision=0.96, Recall=0.99, F1-Score=0.98\n",
      "Klasse B: Precision=0.96, Recall=0.95, F1-Score=0.96\n",
      "Klasse C: Precision=0.74, Recall=0.88, F1-Score=0.80\n",
      "Klasse D: Precision=0.95, Recall=0.97, F1-Score=0.96\n",
      "Klasse E: Precision=0.97, Recall=0.97, F1-Score=0.97\n",
      "Klasse F: Precision=0.79, Recall=0.87, F1-Score=0.83\n",
      "Klasse G: Precision=0.94, Recall=0.94, F1-Score=0.94\n",
      "Klasse H: Precision=0.96, Recall=0.98, F1-Score=0.97\n",
      "Klasse I: Precision=0.60, Recall=0.61, F1-Score=0.61\n",
      "Klasse J: Precision=0.87, Recall=0.84, F1-Score=0.86\n",
      "Klasse K: Precision=0.73, Recall=0.75, F1-Score=0.74\n",
      "Klasse L: Precision=0.90, Recall=0.96, F1-Score=0.93\n",
      "Klasse M: Precision=0.76, Recall=0.89, F1-Score=0.82\n",
      "Klasse a: Precision=0.94, Recall=0.95, F1-Score=0.95\n",
      "Klasse b: Precision=0.94, Recall=0.94, F1-Score=0.94\n",
      "Klasse c: Precision=0.83, Recall=0.69, F1-Score=0.75\n",
      "Klasse d: Precision=0.98, Recall=0.99, F1-Score=0.98\n",
      "Klasse e: Precision=0.98, Recall=0.98, F1-Score=0.98\n",
      "Klasse f: Precision=0.81, Recall=0.74, F1-Score=0.78\n",
      "Klasse g: Precision=0.88, Recall=0.78, F1-Score=0.83\n",
      "Klasse h: Precision=0.98, Recall=0.98, F1-Score=0.98\n",
      "Klasse i: Precision=0.82, Recall=0.72, F1-Score=0.77\n",
      "Klasse j: Precision=0.80, Recall=0.79, F1-Score=0.79\n",
      "Klasse k: Precision=0.73, Recall=0.70, F1-Score=0.71\n",
      "Klasse l: Precision=0.50, Recall=0.47, F1-Score=0.49\n",
      "Klasse m: Precision=0.86, Recall=0.72, F1-Score=0.79\n",
      "\n",
      "Durchschnittlicher F1-Score (gewichtet): 0.87\n",
      "\n",
      "Konfusionsmatrix:\n",
      "[[971   0   0 ...   0   0   0]\n",
      " [  0 591   1 ...   0 281   0]\n",
      " [  0   0 967 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 698   0   1]\n",
      " [  0 322   0 ...   0 472   0]\n",
      " [  0   0   0 ...   0   0 722]]\n",
      "\n",
      "Durchschnittliche ROC-AUC: 1.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87.14444444444445"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "evaluate_model(model, test_loader, device, class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e153ab6",
   "metadata": {},
   "source": [
    "# 1.3 Modularer Klassifikator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3291605",
   "metadata": {},
   "source": [
    "Im folgenden Wird ein Modularer Klassifikator aus unserem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69830861",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_type_map = create_class_type_map(class_list)\n",
    "\n",
    "y_train_type = create_type_labels(y_train, class_list)\n",
    "y_test_type = create_type_labels(y_test, class_list)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train, y_train_type)\n",
    "test_dataset = TensorDataset(X_test, y_test, y_test_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723775f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "# -----------------------------\n",
    "# Optuna-Studie starten\n",
    "# -----------------------------\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\",\n",
    "                             storage=\"sqlite:///optuna_studies.db\",\n",
    "                            study_name=\"Type_Classifier_hyperparam_search11\")\n",
    "study.optimize(get_objective(\n",
    "          train_dataset=train_dataset,\n",
    "          test_dataset=test_dataset,\n",
    "          device=device,\n",
    "          model= TypeClassifier().to(device),\n",
    "          early_stopping=EarlyStopping(),\n",
    "          train_fn=train_one_epoch_type,\n",
    "          eval_fn=evaluate_model_type,\n",
    "          compute_accuracy_fn=compute_accuracy_type), n_trials=20)\n",
    "\n",
    "# Beste Parameter anzeigen\n",
    "print(\"🎯 Beste Hyperparameter:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "study = optuna.load_study(\n",
    "    study_name=\"Type_Classifier_hyperparam_search11\",\n",
    "    storage=\"sqlite:///optuna_studies.db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6386e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Parameter fü Finales Training \n",
    "# -----------------------------\n",
    "best_params = study.best_params\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "model =TypeClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=best_params[\"lr\"], momentum=best_params[\"momentum\"])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=best_params[\"step_size\"], gamma=best_params[\"gamma\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a42f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e9d4f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Epoch 1, Step 200/2813: Batch Loss = 0.8170\n",
      "🔁 Epoch 1, Step 400/2813: Batch Loss = 0.7219\n",
      "🔁 Epoch 1, Step 600/2813: Batch Loss = 0.7777\n",
      "🔁 Epoch 1, Step 800/2813: Batch Loss = 0.6681\n",
      "🔁 Epoch 1, Step 1000/2813: Batch Loss = 0.7169\n",
      "🔁 Epoch 1, Step 1200/2813: Batch Loss = 0.6991\n",
      "🔁 Epoch 1, Step 1400/2813: Batch Loss = 0.5844\n",
      "🔁 Epoch 1, Step 1600/2813: Batch Loss = 0.5243\n",
      "🔁 Epoch 1, Step 1800/2813: Batch Loss = 0.4641\n",
      "🔁 Epoch 1, Step 2000/2813: Batch Loss = 0.4778\n",
      "🔁 Epoch 1, Step 2200/2813: Batch Loss = 0.4337\n",
      "🔁 Epoch 1, Step 2400/2813: Batch Loss = 0.5394\n",
      "🔁 Epoch 1, Step 2600/2813: Batch Loss = 0.5580\n",
      "🔁 Epoch 1, Step 2800/2813: Batch Loss = 0.4370\n",
      "📊 Epoch 1: Train Acc = 71.62%, Val Acc = 78.28%, Val Loss = 0.4942, LR = [0.0003585383114336243]\n",
      "Validation loss decreased (inf --> 0.4942). Saving model state.\n",
      "🔁 Epoch 2, Step 200/2813: Batch Loss = 0.5477\n",
      "🔁 Epoch 2, Step 400/2813: Batch Loss = 0.6006\n",
      "🔁 Epoch 2, Step 600/2813: Batch Loss = 0.4866\n",
      "🔁 Epoch 2, Step 800/2813: Batch Loss = 0.3613\n",
      "🔁 Epoch 2, Step 1000/2813: Batch Loss = 0.4635\n",
      "🔁 Epoch 2, Step 1200/2813: Batch Loss = 0.6332\n",
      "🔁 Epoch 2, Step 1400/2813: Batch Loss = 0.4951\n",
      "🔁 Epoch 2, Step 1600/2813: Batch Loss = 0.4948\n",
      "🔁 Epoch 2, Step 1800/2813: Batch Loss = 0.4156\n",
      "🔁 Epoch 2, Step 2000/2813: Batch Loss = 0.4708\n",
      "🔁 Epoch 2, Step 2200/2813: Batch Loss = 0.5308\n",
      "🔁 Epoch 2, Step 2400/2813: Batch Loss = 0.4178\n",
      "🔁 Epoch 2, Step 2600/2813: Batch Loss = 0.4328\n",
      "🔁 Epoch 2, Step 2800/2813: Batch Loss = 0.6045\n",
      "📊 Epoch 2: Train Acc = 79.11%, Val Acc = 81.45%, Val Loss = 0.4226, LR = [0.0003585383114336243]\n",
      "Validation loss decreased (0.4942 --> 0.4226). Saving model state.\n",
      "🔁 Epoch 3, Step 200/2813: Batch Loss = 0.5211\n",
      "🔁 Epoch 3, Step 400/2813: Batch Loss = 0.3580\n",
      "🔁 Epoch 3, Step 600/2813: Batch Loss = 0.4180\n",
      "🔁 Epoch 3, Step 800/2813: Batch Loss = 0.4102\n",
      "🔁 Epoch 3, Step 1000/2813: Batch Loss = 0.6210\n",
      "🔁 Epoch 3, Step 1200/2813: Batch Loss = 0.3848\n",
      "🔁 Epoch 3, Step 1400/2813: Batch Loss = 0.5242\n",
      "🔁 Epoch 3, Step 1600/2813: Batch Loss = 0.3445\n",
      "🔁 Epoch 3, Step 1800/2813: Batch Loss = 0.4113\n",
      "🔁 Epoch 3, Step 2000/2813: Batch Loss = 0.4189\n",
      "🔁 Epoch 3, Step 2200/2813: Batch Loss = 0.4848\n",
      "🔁 Epoch 3, Step 2400/2813: Batch Loss = 0.3807\n",
      "🔁 Epoch 3, Step 2600/2813: Batch Loss = 0.3896\n",
      "🔁 Epoch 3, Step 2800/2813: Batch Loss = 0.3868\n",
      "📊 Epoch 3: Train Acc = 81.15%, Val Acc = 82.28%, Val Loss = 0.3964, LR = [0.00020337074987141052]\n",
      "Validation loss decreased (0.4226 --> 0.3964). Saving model state.\n",
      "🔁 Epoch 4, Step 200/2813: Batch Loss = 0.4995\n",
      "🔁 Epoch 4, Step 400/2813: Batch Loss = 0.3998\n",
      "🔁 Epoch 4, Step 600/2813: Batch Loss = 0.3554\n",
      "🔁 Epoch 4, Step 800/2813: Batch Loss = 0.3892\n",
      "🔁 Epoch 4, Step 1000/2813: Batch Loss = 0.3865\n",
      "🔁 Epoch 4, Step 1200/2813: Batch Loss = 0.4523\n",
      "🔁 Epoch 4, Step 1400/2813: Batch Loss = 0.3006\n",
      "🔁 Epoch 4, Step 1600/2813: Batch Loss = 0.3648\n",
      "🔁 Epoch 4, Step 1800/2813: Batch Loss = 0.5019\n",
      "🔁 Epoch 4, Step 2000/2813: Batch Loss = 0.2296\n",
      "🔁 Epoch 4, Step 2200/2813: Batch Loss = 0.3423\n",
      "🔁 Epoch 4, Step 2400/2813: Batch Loss = 0.4519\n",
      "🔁 Epoch 4, Step 2600/2813: Batch Loss = 0.4863\n",
      "🔁 Epoch 4, Step 2800/2813: Batch Loss = 0.4286\n",
      "📊 Epoch 4: Train Acc = 82.39%, Val Acc = 83.25%, Val Loss = 0.3774, LR = [0.00020337074987141052]\n",
      "Validation loss decreased (0.3964 --> 0.3774). Saving model state.\n",
      "🔁 Epoch 5, Step 200/2813: Batch Loss = 0.4100\n",
      "🔁 Epoch 5, Step 400/2813: Batch Loss = 0.4291\n",
      "🔁 Epoch 5, Step 600/2813: Batch Loss = 0.4134\n",
      "🔁 Epoch 5, Step 800/2813: Batch Loss = 0.3715\n",
      "🔁 Epoch 5, Step 1000/2813: Batch Loss = 0.3828\n",
      "🔁 Epoch 5, Step 1200/2813: Batch Loss = 0.3530\n",
      "🔁 Epoch 5, Step 1400/2813: Batch Loss = 0.3080\n",
      "🔁 Epoch 5, Step 1600/2813: Batch Loss = 0.3247\n",
      "🔁 Epoch 5, Step 1800/2813: Batch Loss = 0.5704\n",
      "🔁 Epoch 5, Step 2000/2813: Batch Loss = 0.4442\n",
      "🔁 Epoch 5, Step 2200/2813: Batch Loss = 0.2893\n",
      "🔁 Epoch 5, Step 2400/2813: Batch Loss = 0.4867\n",
      "🔁 Epoch 5, Step 2600/2813: Batch Loss = 0.3559\n",
      "🔁 Epoch 5, Step 2800/2813: Batch Loss = 0.4106\n",
      "📊 Epoch 5: Train Acc = 82.93%, Val Acc = 83.41%, Val Loss = 0.3726, LR = [0.00020337074987141052]\n",
      "Validation loss decreased (0.3774 --> 0.3726). Saving model state.\n",
      "🔁 Epoch 6, Step 200/2813: Batch Loss = 0.3149\n",
      "🔁 Epoch 6, Step 400/2813: Batch Loss = 0.3252\n",
      "🔁 Epoch 6, Step 600/2813: Batch Loss = 0.3898\n",
      "🔁 Epoch 6, Step 800/2813: Batch Loss = 0.3276\n",
      "🔁 Epoch 6, Step 1000/2813: Batch Loss = 0.3273\n",
      "🔁 Epoch 6, Step 1200/2813: Batch Loss = 0.3527\n",
      "🔁 Epoch 6, Step 1400/2813: Batch Loss = 0.4122\n",
      "🔁 Epoch 6, Step 1600/2813: Batch Loss = 0.3462\n",
      "🔁 Epoch 6, Step 1800/2813: Batch Loss = 0.3053\n",
      "🔁 Epoch 6, Step 2000/2813: Batch Loss = 0.4257\n",
      "🔁 Epoch 6, Step 2200/2813: Batch Loss = 0.4324\n",
      "🔁 Epoch 6, Step 2400/2813: Batch Loss = 0.4626\n",
      "🔁 Epoch 6, Step 2600/2813: Batch Loss = 0.3344\n",
      "🔁 Epoch 6, Step 2800/2813: Batch Loss = 0.3030\n",
      "📊 Epoch 6: Train Acc = 83.41%, Val Acc = 83.83%, Val Loss = 0.3620, LR = [0.00011535632478962202]\n",
      "Validation loss decreased (0.3726 --> 0.3620). Saving model state.\n",
      "🔁 Epoch 7, Step 200/2813: Batch Loss = 0.2907\n",
      "🔁 Epoch 7, Step 400/2813: Batch Loss = 0.4355\n",
      "🔁 Epoch 7, Step 600/2813: Batch Loss = 0.3740\n",
      "🔁 Epoch 7, Step 800/2813: Batch Loss = 0.2193\n",
      "🔁 Epoch 7, Step 1000/2813: Batch Loss = 0.3381\n",
      "🔁 Epoch 7, Step 1200/2813: Batch Loss = 0.3942\n",
      "🔁 Epoch 7, Step 1400/2813: Batch Loss = 0.3774\n",
      "🔁 Epoch 7, Step 1600/2813: Batch Loss = 0.3064\n",
      "🔁 Epoch 7, Step 1800/2813: Batch Loss = 0.3108\n",
      "🔁 Epoch 7, Step 2000/2813: Batch Loss = 0.4175\n",
      "🔁 Epoch 7, Step 2200/2813: Batch Loss = 0.3261\n",
      "🔁 Epoch 7, Step 2400/2813: Batch Loss = 0.3093\n",
      "🔁 Epoch 7, Step 2600/2813: Batch Loss = 0.3180\n",
      "🔁 Epoch 7, Step 2800/2813: Batch Loss = 0.2551\n",
      "📊 Epoch 7: Train Acc = 83.94%, Val Acc = 84.27%, Val Loss = 0.3555, LR = [0.00011535632478962202]\n",
      "Validation loss decreased (0.3620 --> 0.3555). Saving model state.\n",
      "🔁 Epoch 8, Step 200/2813: Batch Loss = 0.3631\n",
      "🔁 Epoch 8, Step 400/2813: Batch Loss = 0.4174\n",
      "🔁 Epoch 8, Step 600/2813: Batch Loss = 0.2772\n",
      "🔁 Epoch 8, Step 800/2813: Batch Loss = 0.3425\n",
      "🔁 Epoch 8, Step 1000/2813: Batch Loss = 0.4752\n",
      "🔁 Epoch 8, Step 1200/2813: Batch Loss = 0.3455\n",
      "🔁 Epoch 8, Step 1400/2813: Batch Loss = 0.5374\n",
      "🔁 Epoch 8, Step 1600/2813: Batch Loss = 0.4129\n",
      "🔁 Epoch 8, Step 1800/2813: Batch Loss = 0.3844\n",
      "🔁 Epoch 8, Step 2000/2813: Batch Loss = 0.4269\n",
      "🔁 Epoch 8, Step 2200/2813: Batch Loss = 0.3598\n",
      "🔁 Epoch 8, Step 2400/2813: Batch Loss = 0.3967\n",
      "🔁 Epoch 8, Step 2600/2813: Batch Loss = 0.4117\n",
      "🔁 Epoch 8, Step 2800/2813: Batch Loss = 0.2500\n",
      "📊 Epoch 8: Train Acc = 84.04%, Val Acc = 84.34%, Val Loss = 0.3525, LR = [0.00011535632478962202]\n",
      "Validation loss decreased (0.3555 --> 0.3525). Saving model state.\n",
      "🔁 Epoch 9, Step 200/2813: Batch Loss = 0.2365\n",
      "🔁 Epoch 9, Step 400/2813: Batch Loss = 0.4307\n",
      "🔁 Epoch 9, Step 600/2813: Batch Loss = 0.3577\n",
      "🔁 Epoch 9, Step 800/2813: Batch Loss = 0.3535\n",
      "🔁 Epoch 9, Step 1000/2813: Batch Loss = 0.2854\n",
      "🔁 Epoch 9, Step 1200/2813: Batch Loss = 0.2400\n",
      "🔁 Epoch 9, Step 1400/2813: Batch Loss = 0.3369\n",
      "🔁 Epoch 9, Step 1600/2813: Batch Loss = 0.2954\n",
      "🔁 Epoch 9, Step 1800/2813: Batch Loss = 0.3298\n",
      "🔁 Epoch 9, Step 2000/2813: Batch Loss = 0.2885\n",
      "🔁 Epoch 9, Step 2200/2813: Batch Loss = 0.3175\n",
      "🔁 Epoch 9, Step 2400/2813: Batch Loss = 0.5236\n",
      "🔁 Epoch 9, Step 2600/2813: Batch Loss = 0.3668\n",
      "🔁 Epoch 9, Step 2800/2813: Batch Loss = 0.2513\n",
      "📊 Epoch 9: Train Acc = 84.29%, Val Acc = 84.43%, Val Loss = 0.3500, LR = [6.543262331177277e-05]\n",
      "Validation loss decreased (0.3525 --> 0.3500). Saving model state.\n",
      "🔁 Epoch 10, Step 200/2813: Batch Loss = 0.3354\n",
      "🔁 Epoch 10, Step 400/2813: Batch Loss = 0.3988\n",
      "🔁 Epoch 10, Step 600/2813: Batch Loss = 0.2808\n",
      "🔁 Epoch 10, Step 800/2813: Batch Loss = 0.2486\n",
      "🔁 Epoch 10, Step 1000/2813: Batch Loss = 0.3095\n",
      "🔁 Epoch 10, Step 1200/2813: Batch Loss = 0.3337\n",
      "🔁 Epoch 10, Step 1400/2813: Batch Loss = 0.4525\n",
      "🔁 Epoch 10, Step 1600/2813: Batch Loss = 0.4183\n",
      "🔁 Epoch 10, Step 1800/2813: Batch Loss = 0.3221\n",
      "🔁 Epoch 10, Step 2000/2813: Batch Loss = 0.3349\n",
      "🔁 Epoch 10, Step 2200/2813: Batch Loss = 0.4228\n",
      "🔁 Epoch 10, Step 2400/2813: Batch Loss = 0.2769\n",
      "🔁 Epoch 10, Step 2600/2813: Batch Loss = 0.2766\n",
      "🔁 Epoch 10, Step 2800/2813: Batch Loss = 0.3820\n",
      "📊 Epoch 10: Train Acc = 84.55%, Val Acc = 84.80%, Val Loss = 0.3451, LR = [6.543262331177277e-05]\n",
      "Validation loss decreased (0.3500 --> 0.3451). Saving model state.\n",
      "🔁 Epoch 11, Step 200/2813: Batch Loss = 0.4086\n",
      "🔁 Epoch 11, Step 400/2813: Batch Loss = 0.3718\n",
      "🔁 Epoch 11, Step 600/2813: Batch Loss = 0.3974\n",
      "🔁 Epoch 11, Step 800/2813: Batch Loss = 0.3422\n",
      "🔁 Epoch 11, Step 1000/2813: Batch Loss = 0.3270\n",
      "🔁 Epoch 11, Step 1200/2813: Batch Loss = 0.3640\n",
      "🔁 Epoch 11, Step 1400/2813: Batch Loss = 0.4581\n",
      "🔁 Epoch 11, Step 1600/2813: Batch Loss = 0.4323\n",
      "🔁 Epoch 11, Step 1800/2813: Batch Loss = 0.5514\n",
      "🔁 Epoch 11, Step 2000/2813: Batch Loss = 0.2887\n",
      "🔁 Epoch 11, Step 2200/2813: Batch Loss = 0.3657\n",
      "🔁 Epoch 11, Step 2400/2813: Batch Loss = 0.2883\n",
      "🔁 Epoch 11, Step 2600/2813: Batch Loss = 0.2342\n",
      "🔁 Epoch 11, Step 2800/2813: Batch Loss = 0.3955\n",
      "📊 Epoch 11: Train Acc = 84.71%, Val Acc = 84.63%, Val Loss = 0.3442, LR = [6.543262331177277e-05]\n",
      "EarlyStopping counter: 1/10\n",
      "🔁 Epoch 12, Step 200/2813: Batch Loss = 0.3812\n",
      "🔁 Epoch 12, Step 400/2813: Batch Loss = 0.3037\n",
      "🔁 Epoch 12, Step 600/2813: Batch Loss = 0.3689\n",
      "🔁 Epoch 12, Step 800/2813: Batch Loss = 0.3385\n",
      "🔁 Epoch 12, Step 1000/2813: Batch Loss = 0.2748\n",
      "🔁 Epoch 12, Step 1200/2813: Batch Loss = 0.3593\n",
      "🔁 Epoch 12, Step 1400/2813: Batch Loss = 0.2754\n",
      "🔁 Epoch 12, Step 1600/2813: Batch Loss = 0.3005\n",
      "🔁 Epoch 12, Step 1800/2813: Batch Loss = 0.4028\n",
      "🔁 Epoch 12, Step 2000/2813: Batch Loss = 0.3290\n",
      "🔁 Epoch 12, Step 2200/2813: Batch Loss = 0.5079\n",
      "🔁 Epoch 12, Step 2400/2813: Batch Loss = 0.4051\n",
      "🔁 Epoch 12, Step 2600/2813: Batch Loss = 0.3688\n",
      "🔁 Epoch 12, Step 2800/2813: Batch Loss = 0.3308\n",
      "📊 Epoch 12: Train Acc = 84.82%, Val Acc = 84.66%, Val Loss = 0.3423, LR = [3.711481101073988e-05]\n",
      "Validation loss decreased (0.3451 --> 0.3423). Saving model state.\n",
      "🔁 Epoch 13, Step 200/2813: Batch Loss = 0.4552\n",
      "🔁 Epoch 13, Step 400/2813: Batch Loss = 0.4480\n",
      "🔁 Epoch 13, Step 600/2813: Batch Loss = 0.3749\n",
      "🔁 Epoch 13, Step 800/2813: Batch Loss = 0.3196\n",
      "🔁 Epoch 13, Step 1000/2813: Batch Loss = 0.2907\n",
      "🔁 Epoch 13, Step 1200/2813: Batch Loss = 0.3896\n",
      "🔁 Epoch 13, Step 1400/2813: Batch Loss = 0.3077\n",
      "🔁 Epoch 13, Step 1600/2813: Batch Loss = 0.3805\n",
      "🔁 Epoch 13, Step 1800/2813: Batch Loss = 0.3600\n",
      "🔁 Epoch 13, Step 2000/2813: Batch Loss = 0.2569\n",
      "🔁 Epoch 13, Step 2200/2813: Batch Loss = 0.2863\n",
      "🔁 Epoch 13, Step 2400/2813: Batch Loss = 0.4029\n",
      "🔁 Epoch 13, Step 2600/2813: Batch Loss = 0.3957\n",
      "🔁 Epoch 13, Step 2800/2813: Batch Loss = 0.3362\n",
      "📊 Epoch 13: Train Acc = 84.91%, Val Acc = 84.78%, Val Loss = 0.3421, LR = [3.711481101073988e-05]\n",
      "EarlyStopping counter: 1/10\n",
      "🔁 Epoch 14, Step 200/2813: Batch Loss = 0.3430\n",
      "🔁 Epoch 14, Step 400/2813: Batch Loss = 0.4357\n",
      "🔁 Epoch 14, Step 600/2813: Batch Loss = 0.4960\n",
      "🔁 Epoch 14, Step 800/2813: Batch Loss = 0.3619\n",
      "🔁 Epoch 14, Step 1000/2813: Batch Loss = 0.4117\n",
      "🔁 Epoch 14, Step 1200/2813: Batch Loss = 0.2676\n",
      "🔁 Epoch 14, Step 1400/2813: Batch Loss = 0.2471\n",
      "🔁 Epoch 14, Step 1600/2813: Batch Loss = 0.2388\n",
      "🔁 Epoch 14, Step 1800/2813: Batch Loss = 0.3735\n",
      "🔁 Epoch 14, Step 2000/2813: Batch Loss = 0.3723\n",
      "🔁 Epoch 14, Step 2200/2813: Batch Loss = 0.2298\n",
      "🔁 Epoch 14, Step 2400/2813: Batch Loss = 0.2637\n",
      "🔁 Epoch 14, Step 2600/2813: Batch Loss = 0.3512\n",
      "🔁 Epoch 14, Step 2800/2813: Batch Loss = 0.3052\n",
      "📊 Epoch 14: Train Acc = 85.06%, Val Acc = 84.99%, Val Loss = 0.3405, LR = [3.711481101073988e-05]\n",
      "Validation loss decreased (0.3423 --> 0.3405). Saving model state.\n",
      "🔁 Epoch 15, Step 200/2813: Batch Loss = 0.2982\n",
      "🔁 Epoch 15, Step 400/2813: Batch Loss = 0.3755\n",
      "🔁 Epoch 15, Step 600/2813: Batch Loss = 0.3465\n",
      "🔁 Epoch 15, Step 800/2813: Batch Loss = 0.2858\n",
      "🔁 Epoch 15, Step 1000/2813: Batch Loss = 0.5128\n",
      "🔁 Epoch 15, Step 1200/2813: Batch Loss = 0.3366\n",
      "🔁 Epoch 15, Step 1400/2813: Batch Loss = 0.4679\n",
      "🔁 Epoch 15, Step 1600/2813: Batch Loss = 0.3017\n",
      "🔁 Epoch 15, Step 1800/2813: Batch Loss = 0.3186\n",
      "🔁 Epoch 15, Step 2000/2813: Batch Loss = 0.3136\n",
      "🔁 Epoch 15, Step 2200/2813: Batch Loss = 0.3567\n",
      "🔁 Epoch 15, Step 2400/2813: Batch Loss = 0.5194\n",
      "🔁 Epoch 15, Step 2600/2813: Batch Loss = 0.3853\n",
      "🔁 Epoch 15, Step 2800/2813: Batch Loss = 0.3761\n",
      "📊 Epoch 15: Train Acc = 85.09%, Val Acc = 85.04%, Val Loss = 0.3393, LR = [2.1052330269556747e-05]\n",
      "Validation loss decreased (0.3405 --> 0.3393). Saving model state.\n",
      "🔁 Epoch 16, Step 200/2813: Batch Loss = 0.3742\n",
      "🔁 Epoch 16, Step 400/2813: Batch Loss = 0.4670\n",
      "🔁 Epoch 16, Step 600/2813: Batch Loss = 0.4306\n",
      "🔁 Epoch 16, Step 800/2813: Batch Loss = 0.4525\n",
      "🔁 Epoch 16, Step 1000/2813: Batch Loss = 0.2865\n",
      "🔁 Epoch 16, Step 1200/2813: Batch Loss = 0.2837\n",
      "🔁 Epoch 16, Step 1400/2813: Batch Loss = 0.3067\n",
      "🔁 Epoch 16, Step 1600/2813: Batch Loss = 0.4671\n",
      "🔁 Epoch 16, Step 1800/2813: Batch Loss = 0.2736\n",
      "🔁 Epoch 16, Step 2000/2813: Batch Loss = 0.3288\n",
      "🔁 Epoch 16, Step 2200/2813: Batch Loss = 0.2588\n",
      "🔁 Epoch 16, Step 2400/2813: Batch Loss = 0.2651\n",
      "🔁 Epoch 16, Step 2600/2813: Batch Loss = 0.2251\n",
      "🔁 Epoch 16, Step 2800/2813: Batch Loss = 0.2558\n",
      "📊 Epoch 16: Train Acc = 85.14%, Val Acc = 85.00%, Val Loss = 0.3383, LR = [2.1052330269556747e-05]\n",
      "Validation loss decreased (0.3393 --> 0.3383). Saving model state.\n",
      "🔁 Epoch 17, Step 200/2813: Batch Loss = 0.3552\n",
      "🔁 Epoch 17, Step 400/2813: Batch Loss = 0.2758\n",
      "🔁 Epoch 17, Step 600/2813: Batch Loss = 0.3102\n",
      "🔁 Epoch 17, Step 800/2813: Batch Loss = 0.2855\n",
      "🔁 Epoch 17, Step 1000/2813: Batch Loss = 0.4473\n",
      "🔁 Epoch 17, Step 1200/2813: Batch Loss = 0.4108\n",
      "🔁 Epoch 17, Step 1400/2813: Batch Loss = 0.3407\n",
      "🔁 Epoch 17, Step 1600/2813: Batch Loss = 0.2613\n",
      "🔁 Epoch 17, Step 1800/2813: Batch Loss = 0.3567\n",
      "🔁 Epoch 17, Step 2000/2813: Batch Loss = 0.2799\n",
      "🔁 Epoch 17, Step 2200/2813: Batch Loss = 0.3691\n",
      "🔁 Epoch 17, Step 2400/2813: Batch Loss = 0.2036\n",
      "🔁 Epoch 17, Step 2600/2813: Batch Loss = 0.3575\n",
      "🔁 Epoch 17, Step 2800/2813: Batch Loss = 0.4314\n",
      "📊 Epoch 17: Train Acc = 85.21%, Val Acc = 84.97%, Val Loss = 0.3386, LR = [2.1052330269556747e-05]\n",
      "EarlyStopping counter: 1/10\n",
      "🔁 Epoch 18, Step 200/2813: Batch Loss = 0.2036\n",
      "🔁 Epoch 18, Step 400/2813: Batch Loss = 0.4833\n",
      "🔁 Epoch 18, Step 600/2813: Batch Loss = 0.3252\n",
      "🔁 Epoch 18, Step 800/2813: Batch Loss = 0.3134\n",
      "🔁 Epoch 18, Step 1000/2813: Batch Loss = 0.2418\n",
      "🔁 Epoch 18, Step 1200/2813: Batch Loss = 0.2481\n",
      "🔁 Epoch 18, Step 1400/2813: Batch Loss = 0.2611\n",
      "🔁 Epoch 18, Step 1600/2813: Batch Loss = 0.3527\n",
      "🔁 Epoch 18, Step 1800/2813: Batch Loss = 0.3106\n",
      "🔁 Epoch 18, Step 2000/2813: Batch Loss = 0.3344\n",
      "🔁 Epoch 18, Step 2200/2813: Batch Loss = 0.2893\n",
      "🔁 Epoch 18, Step 2400/2813: Batch Loss = 0.4125\n",
      "🔁 Epoch 18, Step 2600/2813: Batch Loss = 0.2992\n",
      "🔁 Epoch 18, Step 2800/2813: Batch Loss = 0.3161\n",
      "📊 Epoch 18: Train Acc = 85.19%, Val Acc = 85.13%, Val Loss = 0.3376, LR = [1.1941340874677946e-05]\n",
      "EarlyStopping counter: 2/10\n",
      "🔁 Epoch 19, Step 200/2813: Batch Loss = 0.3802\n",
      "🔁 Epoch 19, Step 400/2813: Batch Loss = 0.3561\n",
      "🔁 Epoch 19, Step 600/2813: Batch Loss = 0.4781\n",
      "🔁 Epoch 19, Step 800/2813: Batch Loss = 0.2881\n",
      "🔁 Epoch 19, Step 1000/2813: Batch Loss = 0.2339\n",
      "🔁 Epoch 19, Step 1200/2813: Batch Loss = 0.2010\n",
      "🔁 Epoch 19, Step 1400/2813: Batch Loss = 0.2478\n",
      "🔁 Epoch 19, Step 1600/2813: Batch Loss = 0.3301\n",
      "🔁 Epoch 19, Step 1800/2813: Batch Loss = 0.3019\n",
      "🔁 Epoch 19, Step 2000/2813: Batch Loss = 0.2782\n",
      "🔁 Epoch 19, Step 2200/2813: Batch Loss = 0.3196\n",
      "🔁 Epoch 19, Step 2400/2813: Batch Loss = 0.3054\n",
      "🔁 Epoch 19, Step 2600/2813: Batch Loss = 0.4225\n",
      "🔁 Epoch 19, Step 2800/2813: Batch Loss = 0.2683\n",
      "📊 Epoch 19: Train Acc = 85.23%, Val Acc = 85.07%, Val Loss = 0.3373, LR = [1.1941340874677946e-05]\n",
      "EarlyStopping counter: 3/10\n",
      "🔁 Epoch 20, Step 200/2813: Batch Loss = 0.2980\n",
      "🔁 Epoch 20, Step 400/2813: Batch Loss = 0.3721\n",
      "🔁 Epoch 20, Step 600/2813: Batch Loss = 0.2301\n",
      "🔁 Epoch 20, Step 800/2813: Batch Loss = 0.2639\n",
      "🔁 Epoch 20, Step 1000/2813: Batch Loss = 0.4160\n",
      "🔁 Epoch 20, Step 1200/2813: Batch Loss = 0.2656\n",
      "🔁 Epoch 20, Step 1400/2813: Batch Loss = 0.3385\n",
      "🔁 Epoch 20, Step 1600/2813: Batch Loss = 0.3016\n",
      "🔁 Epoch 20, Step 1800/2813: Batch Loss = 0.4127\n",
      "🔁 Epoch 20, Step 2000/2813: Batch Loss = 0.2870\n",
      "🔁 Epoch 20, Step 2200/2813: Batch Loss = 0.3833\n",
      "🔁 Epoch 20, Step 2400/2813: Batch Loss = 0.3266\n",
      "🔁 Epoch 20, Step 2600/2813: Batch Loss = 0.3465\n",
      "🔁 Epoch 20, Step 2800/2813: Batch Loss = 0.4025\n",
      "📊 Epoch 20: Train Acc = 85.32%, Val Acc = 85.11%, Val Loss = 0.3374, LR = [1.1941340874677946e-05]\n",
      "EarlyStopping counter: 4/10\n",
      "🔁 Epoch 21, Step 200/2813: Batch Loss = 0.3087\n",
      "🔁 Epoch 21, Step 400/2813: Batch Loss = 0.3570\n",
      "🔁 Epoch 21, Step 600/2813: Batch Loss = 0.4417\n",
      "🔁 Epoch 21, Step 800/2813: Batch Loss = 0.3822\n",
      "🔁 Epoch 21, Step 1000/2813: Batch Loss = 0.3393\n",
      "🔁 Epoch 21, Step 1200/2813: Batch Loss = 0.3674\n",
      "🔁 Epoch 21, Step 1400/2813: Batch Loss = 0.4237\n",
      "🔁 Epoch 21, Step 1600/2813: Batch Loss = 0.3667\n",
      "🔁 Epoch 21, Step 1800/2813: Batch Loss = 0.5746\n",
      "🔁 Epoch 21, Step 2000/2813: Batch Loss = 0.3200\n",
      "🔁 Epoch 21, Step 2200/2813: Batch Loss = 0.3729\n",
      "🔁 Epoch 21, Step 2400/2813: Batch Loss = 0.2772\n",
      "🔁 Epoch 21, Step 2600/2813: Batch Loss = 0.3975\n",
      "🔁 Epoch 21, Step 2800/2813: Batch Loss = 0.3023\n",
      "📊 Epoch 21: Train Acc = 85.39%, Val Acc = 85.01%, Val Loss = 0.3371, LR = [6.773388981620635e-06]\n",
      "Validation loss decreased (0.3383 --> 0.3371). Saving model state.\n",
      "🔁 Epoch 22, Step 200/2813: Batch Loss = 0.2526\n",
      "🔁 Epoch 22, Step 400/2813: Batch Loss = 0.3758\n",
      "🔁 Epoch 22, Step 600/2813: Batch Loss = 0.2519\n",
      "🔁 Epoch 22, Step 800/2813: Batch Loss = 0.3931\n",
      "🔁 Epoch 22, Step 1000/2813: Batch Loss = 0.3313\n",
      "🔁 Epoch 22, Step 1200/2813: Batch Loss = 0.2663\n",
      "🔁 Epoch 22, Step 1400/2813: Batch Loss = 0.3904\n",
      "🔁 Epoch 22, Step 1600/2813: Batch Loss = 0.3208\n",
      "🔁 Epoch 22, Step 1800/2813: Batch Loss = 0.2716\n",
      "🔁 Epoch 22, Step 2000/2813: Batch Loss = 0.1946\n",
      "🔁 Epoch 22, Step 2200/2813: Batch Loss = 0.2283\n",
      "🔁 Epoch 22, Step 2400/2813: Batch Loss = 0.2143\n",
      "🔁 Epoch 22, Step 2600/2813: Batch Loss = 0.3179\n",
      "🔁 Epoch 22, Step 2800/2813: Batch Loss = 0.3728\n",
      "📊 Epoch 22: Train Acc = 85.42%, Val Acc = 85.08%, Val Loss = 0.3365, LR = [6.773388981620635e-06]\n",
      "EarlyStopping counter: 1/10\n",
      "🔁 Epoch 23, Step 200/2813: Batch Loss = 0.3274\n",
      "🔁 Epoch 23, Step 400/2813: Batch Loss = 0.3844\n",
      "🔁 Epoch 23, Step 600/2813: Batch Loss = 0.2789\n",
      "🔁 Epoch 23, Step 800/2813: Batch Loss = 0.3377\n",
      "🔁 Epoch 23, Step 1000/2813: Batch Loss = 0.3782\n",
      "🔁 Epoch 23, Step 1200/2813: Batch Loss = 0.3632\n",
      "🔁 Epoch 23, Step 1400/2813: Batch Loss = 0.2291\n",
      "🔁 Epoch 23, Step 1600/2813: Batch Loss = 0.2640\n",
      "🔁 Epoch 23, Step 1800/2813: Batch Loss = 0.2671\n",
      "🔁 Epoch 23, Step 2000/2813: Batch Loss = 0.2656\n",
      "🔁 Epoch 23, Step 2200/2813: Batch Loss = 0.3561\n",
      "🔁 Epoch 23, Step 2400/2813: Batch Loss = 0.3504\n",
      "🔁 Epoch 23, Step 2600/2813: Batch Loss = 0.3455\n",
      "🔁 Epoch 23, Step 2800/2813: Batch Loss = 0.3005\n",
      "📊 Epoch 23: Train Acc = 85.42%, Val Acc = 85.10%, Val Loss = 0.3366, LR = [6.773388981620635e-06]\n",
      "EarlyStopping counter: 2/10\n",
      "🔁 Epoch 24, Step 200/2813: Batch Loss = 0.3698\n",
      "🔁 Epoch 24, Step 400/2813: Batch Loss = 0.4334\n",
      "🔁 Epoch 24, Step 600/2813: Batch Loss = 0.3523\n",
      "🔁 Epoch 24, Step 800/2813: Batch Loss = 0.2596\n",
      "🔁 Epoch 24, Step 1000/2813: Batch Loss = 0.4222\n",
      "🔁 Epoch 24, Step 1200/2813: Batch Loss = 0.4631\n",
      "🔁 Epoch 24, Step 1400/2813: Batch Loss = 0.2904\n",
      "🔁 Epoch 24, Step 1600/2813: Batch Loss = 0.2910\n",
      "🔁 Epoch 24, Step 1800/2813: Batch Loss = 0.3732\n",
      "🔁 Epoch 24, Step 2000/2813: Batch Loss = 0.3013\n",
      "🔁 Epoch 24, Step 2200/2813: Batch Loss = 0.2277\n",
      "🔁 Epoch 24, Step 2400/2813: Batch Loss = 0.3636\n",
      "🔁 Epoch 24, Step 2600/2813: Batch Loss = 0.3226\n",
      "🔁 Epoch 24, Step 2800/2813: Batch Loss = 0.4264\n",
      "📊 Epoch 24: Train Acc = 85.41%, Val Acc = 85.08%, Val Loss = 0.3369, LR = [3.8420139562071724e-06]\n",
      "EarlyStopping counter: 3/10\n",
      "🔁 Epoch 25, Step 200/2813: Batch Loss = 0.3472\n",
      "🔁 Epoch 25, Step 400/2813: Batch Loss = 0.4049\n",
      "🔁 Epoch 25, Step 600/2813: Batch Loss = 0.4798\n",
      "🔁 Epoch 25, Step 800/2813: Batch Loss = 0.4456\n",
      "🔁 Epoch 25, Step 1000/2813: Batch Loss = 0.3458\n",
      "🔁 Epoch 25, Step 1200/2813: Batch Loss = 0.4592\n",
      "🔁 Epoch 25, Step 1400/2813: Batch Loss = 0.2605\n",
      "🔁 Epoch 25, Step 1600/2813: Batch Loss = 0.3389\n",
      "🔁 Epoch 25, Step 1800/2813: Batch Loss = 0.2594\n",
      "🔁 Epoch 25, Step 2000/2813: Batch Loss = 0.2395\n",
      "🔁 Epoch 25, Step 2200/2813: Batch Loss = 0.3572\n",
      "🔁 Epoch 25, Step 2400/2813: Batch Loss = 0.3901\n",
      "🔁 Epoch 25, Step 2600/2813: Batch Loss = 0.3330\n",
      "🔁 Epoch 25, Step 2800/2813: Batch Loss = 0.3316\n",
      "📊 Epoch 25: Train Acc = 85.29%, Val Acc = 85.04%, Val Loss = 0.3363, LR = [3.8420139562071724e-06]\n",
      "EarlyStopping counter: 4/10\n",
      "🔁 Epoch 26, Step 200/2813: Batch Loss = 0.3105\n",
      "🔁 Epoch 26, Step 400/2813: Batch Loss = 0.2413\n",
      "🔁 Epoch 26, Step 600/2813: Batch Loss = 0.3470\n",
      "🔁 Epoch 26, Step 800/2813: Batch Loss = 0.4067\n",
      "🔁 Epoch 26, Step 1000/2813: Batch Loss = 0.3780\n",
      "🔁 Epoch 26, Step 1200/2813: Batch Loss = 0.2883\n",
      "🔁 Epoch 26, Step 1400/2813: Batch Loss = 0.3061\n",
      "🔁 Epoch 26, Step 1600/2813: Batch Loss = 0.3336\n",
      "🔁 Epoch 26, Step 1800/2813: Batch Loss = 0.4111\n",
      "🔁 Epoch 26, Step 2000/2813: Batch Loss = 0.3481\n",
      "🔁 Epoch 26, Step 2200/2813: Batch Loss = 0.4516\n",
      "🔁 Epoch 26, Step 2400/2813: Batch Loss = 0.3209\n",
      "🔁 Epoch 26, Step 2600/2813: Batch Loss = 0.2576\n",
      "🔁 Epoch 26, Step 2800/2813: Batch Loss = 0.3759\n",
      "📊 Epoch 26: Train Acc = 85.50%, Val Acc = 85.08%, Val Loss = 0.3363, LR = [3.8420139562071724e-06]\n",
      "EarlyStopping counter: 5/10\n",
      "🔁 Epoch 27, Step 200/2813: Batch Loss = 0.3072\n",
      "🔁 Epoch 27, Step 400/2813: Batch Loss = 0.2314\n",
      "🔁 Epoch 27, Step 600/2813: Batch Loss = 0.2307\n",
      "🔁 Epoch 27, Step 800/2813: Batch Loss = 0.3516\n",
      "🔁 Epoch 27, Step 1000/2813: Batch Loss = 0.2448\n",
      "🔁 Epoch 27, Step 1200/2813: Batch Loss = 0.3272\n",
      "🔁 Epoch 27, Step 1400/2813: Batch Loss = 0.3667\n",
      "🔁 Epoch 27, Step 1600/2813: Batch Loss = 0.2546\n",
      "🔁 Epoch 27, Step 1800/2813: Batch Loss = 0.3938\n",
      "🔁 Epoch 27, Step 2000/2813: Batch Loss = 0.3754\n",
      "🔁 Epoch 27, Step 2200/2813: Batch Loss = 0.2370\n",
      "🔁 Epoch 27, Step 2400/2813: Batch Loss = 0.2471\n",
      "🔁 Epoch 27, Step 2600/2813: Batch Loss = 0.3261\n",
      "🔁 Epoch 27, Step 2800/2813: Batch Loss = 0.4547\n",
      "📊 Epoch 27: Train Acc = 85.41%, Val Acc = 85.10%, Val Loss = 0.3364, LR = [2.1792741092744506e-06]\n",
      "EarlyStopping counter: 6/10\n",
      "🔁 Epoch 28, Step 200/2813: Batch Loss = 0.3182\n",
      "🔁 Epoch 28, Step 400/2813: Batch Loss = 0.4161\n",
      "🔁 Epoch 28, Step 600/2813: Batch Loss = 0.5042\n",
      "🔁 Epoch 28, Step 800/2813: Batch Loss = 0.4040\n",
      "🔁 Epoch 28, Step 1000/2813: Batch Loss = 0.4549\n",
      "🔁 Epoch 28, Step 1200/2813: Batch Loss = 0.3026\n",
      "🔁 Epoch 28, Step 1400/2813: Batch Loss = 0.3036\n",
      "🔁 Epoch 28, Step 1600/2813: Batch Loss = 0.4260\n",
      "🔁 Epoch 28, Step 1800/2813: Batch Loss = 0.3094\n",
      "🔁 Epoch 28, Step 2000/2813: Batch Loss = 0.4750\n",
      "🔁 Epoch 28, Step 2200/2813: Batch Loss = 0.1868\n",
      "🔁 Epoch 28, Step 2400/2813: Batch Loss = 0.3037\n",
      "🔁 Epoch 28, Step 2600/2813: Batch Loss = 0.3403\n",
      "🔁 Epoch 28, Step 2800/2813: Batch Loss = 0.3458\n",
      "📊 Epoch 28: Train Acc = 85.47%, Val Acc = 85.11%, Val Loss = 0.3365, LR = [2.1792741092744506e-06]\n",
      "EarlyStopping counter: 7/10\n",
      "🔁 Epoch 29, Step 200/2813: Batch Loss = 0.3518\n",
      "🔁 Epoch 29, Step 400/2813: Batch Loss = 0.4698\n",
      "🔁 Epoch 29, Step 600/2813: Batch Loss = 0.3608\n",
      "🔁 Epoch 29, Step 800/2813: Batch Loss = 0.2722\n",
      "🔁 Epoch 29, Step 1000/2813: Batch Loss = 0.4485\n",
      "🔁 Epoch 29, Step 1200/2813: Batch Loss = 0.4299\n",
      "🔁 Epoch 29, Step 1400/2813: Batch Loss = 0.3725\n",
      "🔁 Epoch 29, Step 1600/2813: Batch Loss = 0.3469\n",
      "🔁 Epoch 29, Step 1800/2813: Batch Loss = 0.3258\n",
      "🔁 Epoch 29, Step 2000/2813: Batch Loss = 0.4049\n",
      "🔁 Epoch 29, Step 2200/2813: Batch Loss = 0.4303\n",
      "🔁 Epoch 29, Step 2400/2813: Batch Loss = 0.3914\n",
      "🔁 Epoch 29, Step 2600/2813: Batch Loss = 0.3013\n",
      "🔁 Epoch 29, Step 2800/2813: Batch Loss = 0.4610\n",
      "📊 Epoch 29: Train Acc = 85.40%, Val Acc = 85.12%, Val Loss = 0.3360, LR = [2.1792741092744506e-06]\n",
      "Validation loss decreased (0.3371 --> 0.3360). Saving model state.\n",
      "🔁 Epoch 30, Step 200/2813: Batch Loss = 0.3476\n",
      "🔁 Epoch 30, Step 400/2813: Batch Loss = 0.3178\n",
      "🔁 Epoch 30, Step 600/2813: Batch Loss = 0.3085\n",
      "🔁 Epoch 30, Step 800/2813: Batch Loss = 0.4524\n",
      "🔁 Epoch 30, Step 1000/2813: Batch Loss = 0.3409\n",
      "🔁 Epoch 30, Step 1200/2813: Batch Loss = 0.3292\n",
      "🔁 Epoch 30, Step 1400/2813: Batch Loss = 0.4871\n",
      "🔁 Epoch 30, Step 1600/2813: Batch Loss = 0.3517\n",
      "🔁 Epoch 30, Step 1800/2813: Batch Loss = 0.3963\n",
      "🔁 Epoch 30, Step 2000/2813: Batch Loss = 0.3199\n",
      "🔁 Epoch 30, Step 2200/2813: Batch Loss = 0.3061\n",
      "🔁 Epoch 30, Step 2400/2813: Batch Loss = 0.2664\n",
      "🔁 Epoch 30, Step 2600/2813: Batch Loss = 0.3215\n",
      "🔁 Epoch 30, Step 2800/2813: Batch Loss = 0.2818\n",
      "📊 Epoch 30: Train Acc = 85.40%, Val Acc = 85.11%, Val Loss = 0.3365, LR = [1.236131804175533e-06]\n",
      "EarlyStopping counter: 1/10\n",
      "✅ Bestes Modell wurde wiederhergestellt\n"
     ]
    }
   ],
   "source": [
    "train_type_classifier(train_loader, test_loader, model, device, criterion, optimizer, scheduler, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4614352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modell gespeichert.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.save(model.state_dict(), './type_classifier_best_hyperparams.pth')\n",
    "print(\"✅ Modell gespeichert.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "611a1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teilmodelle laden und einfrieren\n",
    "tm1 = ResNet18(num_classes=36).to(device)\n",
    "tm1.load_state_dict(torch.load(\"resnet18_best_hyperparams.pth\", map_location=device))\n",
    "for param in tm1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in tm1.linear.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "tm2 = TypeClassifier().to(device)\n",
    "tm2.load_state_dict(torch.load(\"type_classifier_best_hyperparams.pth\", map_location=device))\n",
    "for param in tm2.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa98a14",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optuna' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Optuna-Studie starten\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m study = \u001b[43moptuna\u001b[49m.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m                              storage=\u001b[33m\"\u001b[39m\u001b[33msqlite:///optuna_studies.db\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m                             study_name=\u001b[33m\"\u001b[39m\u001b[33mModular_Classifier_hyperparam_search10\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m study.optimize(get_objective(\n\u001b[32m      9\u001b[39m           train_dataset=train_dataset,\n\u001b[32m     10\u001b[39m           test_dataset=test_dataset,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m           eval_fn=evaluate_model_modular,\n\u001b[32m     16\u001b[39m           compute_accuracy_fn=compute_accuracy_modular), n_trials=\u001b[32m20\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Beste Parameter anzeigen\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'optuna' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Optuna-Studie starten\n",
    "# -----------------------------\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\",\n",
    "                             storage=\"sqlite:///optuna_studies.db\",\n",
    "                            study_name=\"Modular_Classifier_hyperparam_search10\")\n",
    "study.optimize(get_objective(\n",
    "          train_dataset=train_dataset,\n",
    "          test_dataset=test_dataset,\n",
    "          device=device,\n",
    "          model= ModularClassifier(tm1,tm2,class_type_map).to(device),\n",
    "          early_stopping=EarlyStopping(),\n",
    "          train_fn=train_one_epoch_modular,\n",
    "          eval_fn=evaluate_model_modular,\n",
    "          compute_accuracy_fn=compute_accuracy_modular), n_trials=20)\n",
    "\n",
    "# Beste Parameter anzeigen\n",
    "print(\"🎯 Beste Hyperparameter:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "tudy = optuna.load_study(\n",
    "    study_name=\"Modular_Classifier_hyperparam_search10\",\n",
    "       storage=\"sqlite:///optuna_studies.db\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d9effd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Parameter für Finales Training \n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "model =ModularClassifier(tm1,tm2,class_type_map).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0018460023232891255, momentum=0.6076939425444646)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma= 0.7858173311049056)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2bc3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_modular_classifier(model, train_loader, test_loader, device, optimizer, scheduler=None, epochs=50):\n",
    "    early_stopping = EarlyStopping(patience=5, delta=0.001, verbose=True)\n",
    "    best_accuracy = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def evaluate(model, loader):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels_cls, labels_type in loader:\n",
    "                images = images.to(device)\n",
    "                labels_cls = labels_cls.to(device)\n",
    "                labels_type = labels_type.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels_cls)\n",
    "\n",
    "                total_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels_cls).sum().item()\n",
    "                total += labels_cls.size(0)\n",
    "\n",
    "        avg_loss = total_loss / total\n",
    "        accuracy = 100.0 * correct / total\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for images, labels_cls, labels_type in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels_cls = labels_cls.to(device)\n",
    "            labels_type = labels_type.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Vorhersagen getrennt berechnen\n",
    "            out_cls = model.tm1(images)\n",
    "            out_type = model.tm2(images)\n",
    "            combined_output = model(images)\n",
    "\n",
    "            # Verluste berechnen\n",
    "            loss_cls = criterion(combined_output, labels_cls)\n",
    "            loss_type = criterion(out_type, labels_type)\n",
    "\n",
    "            # Kombinierter Verlust\n",
    "            loss = loss_cls + 0.5 * loss_type  # Gewichtung optional anpassbar\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(combined_output, 1)\n",
    "            correct_train += (predicted == labels_cls).sum().item()\n",
    "            total_train += labels_cls.size(0)\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = running_loss / total_train\n",
    "        train_acc = 100.0 * correct_train / total_train\n",
    "        val_loss, val_acc = evaluate(model, test_loader)\n",
    "\n",
    "        print(f\"📊 Epoche {epoch+1}/{epochs}: \"\n",
    "              f\"Train Loss = {avg_train_loss:.4f}, Train Acc = {train_acc:.2f}%, \"\n",
    "              f\"Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.2f}%, \"\n",
    "              f\"LR = {scheduler.get_last_lr()[0] if scheduler else 'N/A'}\")\n",
    "\n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"🛑 Early Stopping bei Epoche {epoch+1} (Beste Val Acc: {best_accuracy:.2f}%)\")\n",
    "            break\n",
    "\n",
    "    if early_stopping.best_model_state is not None:\n",
    "        model.load_state_dict(early_stopping.best_model_state)\n",
    "        print(\"✅ Bestes Modell wiederhergestellt.\")\n",
    "    else:\n",
    "        print(\"⚠️ Kein besseres Modell als Initialzustand gefunden.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddd82436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoche 1/50: Train Loss = 3.7637, Train Acc = 2.69%, Val Loss = 3.5882, Val Acc = 2.42%, LR = 0.0018460023232891255\n",
      "Validation loss decreased (inf --> 3.5882). Saving model state.\n",
      "📊 Epoche 2/50: Train Loss = 3.7630, Train Acc = 2.74%, Val Loss = 3.5879, Val Acc = 2.34%, LR = 0.0018460023232891255\n",
      "EarlyStopping counter: 1/5\n",
      "📊 Epoche 3/50: Train Loss = 3.7630, Train Acc = 2.67%, Val Loss = 3.5876, Val Acc = 2.26%, LR = 0.0018460023232891255\n",
      "EarlyStopping counter: 2/5\n",
      "📊 Epoche 4/50: Train Loss = 3.7624, Train Acc = 2.66%, Val Loss = 3.5874, Val Acc = 2.33%, LR = 0.0014506206189005158\n",
      "EarlyStopping counter: 3/5\n",
      "📊 Epoche 5/50: Train Loss = 3.7628, Train Acc = 2.66%, Val Loss = 3.5872, Val Acc = 2.27%, LR = 0.0014506206189005158\n",
      "EarlyStopping counter: 4/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model =ModularClassifier(tm1,tm2,class_type_map).to(device)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrain_modular_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mtrain_modular_classifier\u001b[39m\u001b[34m(model, train_loader, test_loader, device, optimizer, scheduler, epochs)\u001b[39m\n\u001b[32m     54\u001b[39m loss.backward()\n\u001b[32m     55\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * images.size(\u001b[32m0\u001b[39m)\n\u001b[32m     58\u001b[39m _, predicted = torch.max(combined_output, \u001b[32m1\u001b[39m)\n\u001b[32m     59\u001b[39m correct_train += (predicted == labels_cls).sum().item()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model =ModularClassifier(tm1,tm2,class_type_map).to(device)\n",
    "train_modular_classifier(model, train_loader, test_loader, device, optimizer, scheduler, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b87d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'lr': 0.0012540729304007245,\n",
       " 'momentum': 0.6645569703384998,\n",
       " 'step_size': 3,\n",
       " 'gamma': 0.8046924609772251}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
